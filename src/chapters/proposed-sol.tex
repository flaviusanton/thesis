\chapter{Proposed Solution}
\label{chapter:proposed-sol}

In this chapter we present a detailed description of the system we have built
in the scope of this thesis. In \labelindexref{Section}{sec:reqs-tech} we
present what were the requirements and constraints of this project together
with what technologies we used and why we chose them. In the next part of this
chapter, \labelindexref{Section}{sec:sys-arch}, we take each component in the
pipeline and present how it works. Finally, the third part,
\labelindexref{Section}{sec:logo-alg}, consists in the detailed description of
the template matching algorithm variation that we used for the logo
recognition component.

We see in the next section that one of the biggest requirements
(and achievements for that matter) of this thesis was to build a system that
is algorithm agnostic and can do any type of computation on images fetched
from social media streams. In order for this to happen, it was necessary to
divide the work into multiple separate components each one being responsible
for a very specific task, i.e. fetching images, storing images, recognizing
logos etc. But before diving into each component, let us see what were the
requirements and technologies used in building this system.

\section{Requirements and Technologies Used}
\label{sec:reqs-tech}

Even though the project is open source and we had almost full freedom in
designing the system and choosing the technologies, we still had to conform
with some requirements came from the proposing company, Hootsuite. One of the
rigid requirements was that the project has to be written in Scala, not
necessarily because they considered Scala is the best choice here, but because
most, if not all, of their platform is built upon Scala. More details about
Scala, what are its advantages, disadvantages or why it is fun to use, can be
found in \labelindexref{Section}{sub-sec:scala}.

Another requirement that did not come from Hootsuite, but rather from the
common sense is that the system has to scale very easily. Like we saw in
\labelindexref{Section}{sec:proj-motivation}, the number of images that this
system has to face is quite big, even with the computation power we have
available nowadays. The most intuitive way to imagine a scalable system is if
we add an arbitrary number of machines, let's say \(M_{new}\), than the task
has to be completed \(1 + \frac{M_{new}}{M_{old}}\) times faster, where \(M_{old}\) is the old number
of machines available. In practice, mostly due to Amdahl's
Law\footnote{\url{http://home.wlu.edu/~whaleyt/classes/parallel/topics/amdahl.html}},
but also because of other factors, like network latency, hardware or software
fails etc, the system will never reach that idealized speedup, but if the
speed gain is linear with \(1 + \frac{M_{new}}{M_{old}}\), we can declare that the
system is scalable. Fortunately for us, the task we have to perform is very
easy to distribute across multiple machines. Suppose that we have \(M\)
machines and each of them is running an instance of the logo recognition
algorithm, then, if we have \(2 \times M\) images to process, we can easily
distribute \(2\) images per machine and then collect the results in the end.
Because images that come from the social media streaming endpoints are rather
independent and processing them represents perfectly independent tasks, we do
not even need to collect the results, but this is a discussion we will have in
\labelindexref{Section}{sec:sys-arch}.

One of the main characteristics of the system is that it has to allow
introducing new \textit{known} logos or removing old ones in an easy manner.
It is allowed to have a small delay of a few minutes before the system can
recognize the new logo when we want to insert it
in the \textit{known logos database}, but not hours or, even worse,
days. By easy we mean here fast and reliable rather than easy from the end user point of
view. The actual process is fine to involve digging into the filesystem,
updating some directory, restarting some components, but this process has to
be seamless from the whole system perspective, we cannot afford shutting
everything down and restarting just because we want to insert a new logo
because the service has to run continuously.

The process of moving an image through the pipeline from one end to the other
should finish either with a positive result (i.e. we found this list of logos
in the image) or with a negative one. In the former case, one of the
components has to store the image (or the image link) in a database together
with some data to describe the result, for example the logo list. This type of
metadata is often called \textit{annotated data} along this thesis and we may
also refer to the pair \((image, logo list)\) as the \textit{annotated
image}.

Despite being designed as mostly a background job that sometimes stores
results in a database, our system have to have at least a minimum interaction
with a human and we decided that interaction to be in the form of a minimal
web interface where one can basically see the pairs \((image, logo list)\)
from the database, but in a nicer form. Also, the dashboard has to offer at
least a table or chart with statistics regarding recognized logos. To increase
the usefulness of the dashboard it should be real time and compute the
statistics as long as new logos are discovered. It is fine to have a slight
delay in updating the dashboard, but the web application should poll the
database at least once a minute.

Because our focus is rather on the infrastructure and reliability rather than
the algorithm approach itself, which can be changed at any future time, but
also because we cannot miss important data and have false statistics, the system
is not allowed to lose messages. In the scope of this thesis, by
\textit{message} we understand the output of one component and the input of
the next component in the pipeline. Of course, the first component, Fetch
Module (see \labelindexref{Section}{sub-sec:fetch-module}) and the last
component, Annotated Data Storage Module (see
\labelindexref{Section}{sub-sec:ads-module}), make an exception from this rule
as they only output messages, in the case of the former, or consume messages, if we speak
about the latter. That does not mean that the first component has no input, it
means that its input comes directly from the outside world and represent
unstructured data, i.e. tweets or posts, streamed by the Social Networks.
Because the volume of steamed data can become very large, we have to make sure
that all this data gets processed on time by our system, so in order for this
to happen, the first component has to quickly get rid of the posts and pass
the work to the next layers.

When it comes to passing messages around, we needed a good infrastructure that
supports not necessarily large amounts of data, but a tool that allows multiple producers,
multiple consumers and does not offer the same message twice for processing, because we do not
want the same image to be analyzed multiple times. We say that we
do not need large amounts of data, because we do not have to send entire
images between components. We describe in
\labelindexref{Section}{sub-sec:im-store} the way we store images
so that each component can fetch them directly from the centralized storage, thus avoiding
passing them around each time we send a message. Therefore, our messaging
system has to be reliable, seamlessly scalable and have a fast way of
partitioning the data channels, because we do not want to mix messages around,
we only need one channel between two types of components without allowing
other processes to have access to that data. One of the mature, distributed
open source, messaging systems available is Apache
Kafka\footnote{\url{http://kafka.apache.org}}.

\subsection{Kafka -- A Messaging System}
\label{sub-sec:kafka}

"Apache Kafka is a distributed, partitioned, replicated commit log service. It
provides the functionality of a messaging system, but with a unique
design."\cite{kafka}

First, let us be clear with their terminology for some of the bits that are of
direct interest to us. Kafka maintains feeds of messages in categorized queues
called \textit{topics}. These topics are, at the low level view, nothing more
than large buffer caches where some processes write data (\textit{producers})
and other processes read data (\textit{consumers}). Kafka is designed to run
over a cluster of servers, each one of those being called a \textit{broker}, but
for the scope of this thesis it is not necessary to know what a broker is.

The most important abstraction that Kafka provides is the topic because it is
practically the way we can sort and categorize the large amount of data that
we are dealing with. The Kafka cluster maintains, for each topic, a commit log
where data is appended and cached for a configurable amount of time. For
example, if the log retention is set up at one day, the messages received into
one Kafka topic are kept for 24 hours, after which they will be deleted in
order to free disk space.

The two main types of active entities in a Kafka environment are the consumers
and the producers. A producer is responsible for connecting to one Kafka
broker and sending the message to the correct topic, but also for choosing the
right \textit{partition} to which it wants to send the message.

The consumers are a little bit more special. Kafka introduces another abstraction
here, namely the \textit{consumer group} which is nothing more than several
consumers grouped by a common identifier. What is actually
interesting here is that, by this approach, one can choose from two models of
usage: message queue and publish-subscribe model. What Kafka does is that all
the messages from one partition go to all consumer groups (publish-subscribe
model), but only one consumer from a consumer group can read a given message
(queueing model). This proved, over time, to be extremely useful and it is,
indeed, in our case also. \labelindexref{Figure}{img:kafka-arch} offers a
visual interpretation of the two models that Kafka provides. It is a two
server Kafka cluster hosting 4 partitions with 2 consumer groups. Consumer
group A has 2 consumers, whereas consumer group B has 4 consumers. The
interesting part is that each partition is consumed by both consumer groups
(load balancing), but only one consumer from each consumer group reads the
message (uniqueness). The good part here is that Kafka is able to provide both
load balancing and ordering in the scope of the same partition.

\fig[scale=0.7]{src/img/kafka-arch.png}{img:kafka-arch}{Kafka
Cluster\cite{fig31}}

\subsection{Scala}
\label{sub-sec:scala}

Scala is a programming language created by a team conducted by prof. Martin
Odersky at the \'Ecole Polytechnique F\'ed\'erale de Lausanne (EPFL), Switzerland.
"Scala fuses object-oriented and functional programming in a statically typed
programming language. It is aimed at the construction of components and
component systems."\cite{scala}

As prof. Martin Odersky et al state in their paper\cite{scala}, Scala is a language designed for
building components. And "true component systems have been an elusive goal
of the software industry.
Ideally, software should be assembled from libraries of pre-written
components, just as hardware is assembled from pre-fabricated chips. In
reality, large parts of software applications are written from scratch, so
that software production is still more a craft than an industry."

One of the main goals of Scala is to provide a \textit{scalable} language in
the sense that the same concepts can describe small as well as large parts.
Therefore, their main focus is on building upon composition and decomposition
and concentrate on abstraction, rather than adding a large set of primitives
that might be useful in the scope of one component at the bare code level, but
not at other levels.

Apart from these thoretical considerations, Scala has several characteristics
that make it not only suitable for industry applications, but also fun to use.
Among them, one can count:
\begin{itemize}
  \item seamless Java interoperability: Scala runs on the JVM, so Java and Scala
  stacks can be freely mixed for totally seamless integration.
  \item type inference: Scala has static types, but one does not have to
  specify types for every variable they use, the Scala type system being
  clever enough to deduct almost all of them.
  \item concurrency and distribution: Scala uses data-parallel operations on
  collections, actors for concurrency and distribution, or futures for
  asynchronous programming.
  \item traits: combines the flexibility of Java-style interfaces with the
  power of classes. Think at principled multiple-inheritance.
  \item pattern matching: match against class hierarchies, sequences and more.
  Basically, one can use any type of variable in a \texttt{switch}-like
  manner.
  \item higher-order functions: functions are first-class objects in Scala.
  You can compose them with guaranteed type safety and use them anywhere or pass
  them to anything.
\end{itemize}

To better understand the constrast between Scala and Java (we are directly
comparing these two languages because they are very related as they both run
over the JVM), let's look at \labelindexref{Listing}{lst:prod-java}, which is a
Java method that returns a copy of the list of products in an object, and
compare it with \labelindexref{Listing}{lst:prod-scala} which is the
equivalent Scala code.

\lstset{language=Java,caption=Code written in Java,label=lst:prod-java}
\begin{lstlisting}
public List<Product> getProducts() {
  List<Product> products = new ArrayList<Product>();
  for (Order order : orders) {
    products.addAll(order.getProducts());
  }
  return products;
\end{lstlisting}

\lstset{language=Scala,caption=Code written in Scala,label=lst:prod-scala}
\begin{lstlisting}
def products = orders.flatMap(o => o.products)
\end{lstlisting}

\section{System Architecture}
\label{sec:sys-arch}

In this section we explore the system architecture as a whole and then we go
one level up and explore each separate component.
\labelindexref{Figure}{img:hl-arch} shows a general overview of the whole
pipeline. As shown, data comes from the social networks and enters the
pipeline through the Fetcher Module which filters every post or tweet and
writes the output to a Kafka topic (see
\labelindexref{Section}{sub-sec:kafka}) from which several processes read and
download the images. The downloaded data then gets sent by the Retrievers to
the centralized Image Store from which the Detectors pull images for
processing purposes. The Detectors are also connected with the Retrievers by a
Kafka topic. After the detection process, the life of an image can stop, in
case nothing was detected, or can continue to the Storage Module which talks
to a MongoDB\footnote{\url{https://www.mongodb.org}} database. The database
gets polled periodically by a web application which can be accessed directly by a human
user.

\fig[scale=0.35]{src/img/hl-arch.pdf}{img:hl-arch}{High Level System
Architecture}

Even though some of the components may seem that could have been very well
integrated into other ones (for example Image Retriever into Fetcher), we will
shortly see that if we do some computations there is impossible to use only
one machine for some of them, in our case for the Image Retriever component.
Therefore, the reason why some of the modules are drawn as duplicated
rectangles is to emphasize that they are replicated processes, so not
necessarily need to be run on different machines, but there is a need for more
instances of them. The replication is not needed, in this case, for
reliability and fault tolerance purposes, but in order to increase the power
of consuming more incoming data.

Apart from the replicated components, \labelindexref{Figure}{img:hl-arch}
shows another strange characteristic: there are two piles of Logo Detectors.
By this, we only try to show that some detectors are specialized in recognizing
some logos, while other pile of detectors are specialized in another set of
logos. We took this decision because the process of testing an image against
each and every possible logo in the database would take too long on a single
detector, so it is better to have an even work distribution. Moreover, the most
important reason we chose to do this is that, if we want to add a new logo in
the equation, we only have to restart one pile of detectors, which may be
considerably faster than restarting all detectors.

As we have seen in \labelindexref{Section}{sub-sec:kafka}, the messaging
system we chose for our implementation can keep messages for as long as they
are needed and producers or consumers can come and go at any given time
without impacting the stored data. To use this to our advantage, each
component basically runs an infinite loop where it waits for input, do some
computation and writes to the corresponding queue towards the next component
in the chain, as the code in \labelindexref{Listing}{lst:comp-workflow} shows.

\lstset{caption=General Component Workflow,label=lst:comp-workflow}
\begin{lstlisting}
initializeComponent();
consumer = new KafkaConsumer(INPUT_TOPIC);
producer = new KafkaProducer(OUTPUT_TOPIC);

while (true)
  message = consumer.consumeOne();
  result = process(message);
  producer.produce(result);
\end{lstlisting}

Having this type of workflow for each component is also very convenient from
the update or upgrade perspective. Basically, each component is a
stateless process that takes some input, applies a function that does
not need any state and then returns an output. The only entity responsible for
keeping the state is Kafka, our distributed messaging system, so fortunately
for us, we can turn down and up any component at any given time. In a
hypothetic situation where all Detectors have failed and went down, the system
is well able to recover if the Detectors restart, providing there is enough disk space
to cache all those incoming messages and images. However, disk have become so
cheap nowadays that we can afford having a lot of it, such that the probability of
complete system fail (data loss) is very low.

As far as component restarts concern, they are also needed in case of a new
logo being added to the known logo database or in case of a code base update.
It is true that in the latter situation, we also have to make sure the changes
are compatible, but the biggest concern should be how do we keep the whole
system running, in spite of updating some of the components. This problem is
also automatically solved by the fact that our clever messaging system is
caching for as long as we need it to. If a consumer simply goes down for
whatever reason, Kafka will notice that, but there is no action required as
the remaining consumers will simply take that workload. If there is no
consumer left or too few of them, Kafka will cache the messages until there
is again a sufficient number of consumers to continue working.

That being said, in the following sections, we present each separate component that takes part
into the processing pipeline.

\subsection{Fetch Module}
\label{sub-sec:fetch-module}

The Fetch Module is the frontend of our system as it is the only component
that connects to the outside world. It is responsible for fetching streams of
unstructured data, i.e. posts, tweets etc, filtering this data in order to
keep only what is of interest to us, namely images and downloading these
images. After that, the downloaded image gets stored by the Image Store module
and the Detectors are notified by the incoming message in the corresponding
Kafka queue. This is what happens in a few words, but let us explore some
of the subtleties of this module.

First of all, there is no need to have multiple machines, nor processes that
are responsible for fetching posts or tweets from the streaming endpoints. To
back this up, we make a reference to
\labelindexref{Section}{sec:proj-objectives} and the calculations we did
there. If we sum all the incoming tweets and posts we barely reach around 250
messages per second. Given the fact that images are not included as data
inside the tweets, but as links and we know that a tweet has at most 140
characters (let us assume that all the data comes from Twitter and a link has
at most 100 bytes for simplicity) simple math yields that this component has
to face 60KB per second (\((140B + 100B) \times 250 = 60KB\)), a value that
was modest even 10 years ago.

One may simply ask what happens with this component in case of hardware or
software failures and it is a valid question. Well, the answer is so simple
that it may be shocking at the first glance: nothing happens immediately. By
this, we do not mean that the system stops working entirely, but for a period
of time, the fetch component will simply be down and there will be no new
tweets fetched. We chose to do this mainly for two reasons. First, because you
cannot have multiple connections to the same social network endpoint. The
Twitter APIs, for example, limit the number of connections at 1 per
application, so there is no point in having replicas for this component.
Second, because even if we would have additional replicas being ready to jump in when the
main replica fails, there is still a number of streamed tweets that will be
lost forever. With this in mind and the fact that Twitter sends us only 1\%
of their data, there is nothing to worry about if we lose a number of streamed
tweets, because we do not have control of which tweets they stream. Those
\(N\) tweets that we presumably cannot afford to lose could have been in the
other 99\% from the beginning. The most important thing about our system is to
keep the Detectors busy all the time, because they are the real bottleneck of
the whole pipeline, even if the Detectors have the biggest number of machines
assigned. Due to cost limitations, (see
\labelindexref{Section}{sec:cost-and-resource}) we periodically need to shut
down the Fetch Module, to allow the Detectors to keep up with all the incoming
data.

In the future, if the hardware power is increased considerably, there is also
the possibility of adding dedicated Fetcher replicas that are ready to jump in
the game in case of a failure, but for now this is not a big concern.
Moreover, if we add more social networks to the system such that one single
Fetcher has to download from too many endpoints, we might also change the
design and split the work between multiple Fetchers.

As we have stated before, the Fetcher is only responsible for getting the
stream of data and filter the images and by filtering means discarding the
whole message content but the image URL. The image URL is the only thing that
gets past the Fetcher and gets written in a corresponding Kafka queue on which
more Image Retriever processes await.

It should be clear to the reader, by now, that the Fetcher and Image Retriever
(see \labelindexref{Figure}{img:hl-arch}) are two separate processes, but they
do not need to run on different machines. Because the processing power needed here is
very low, but also because these two components are very bound to each other
in terms of logic, we are considering these two types of processes as
being one single component. Even though there is a Kafka queue between them,
this is used more as a caching level rather than as a logical boundary.

The Image Retriever processes are, therefore, replicated mostly regarding
failure resistance, but also because of bandwidth. If we know that an image is
approximately 50--200KB and we have about 30 images per second to process that
gives us \(30 \times 200KB = 6MB\) of data to download per second in the worst
case. If we also take peaks due to busy hours into consideration, the amount
of data to download in one second can exceed 10MB so we do not want all this
incoming traffic on one single machine. This is the main reason we should have at
least 2 or 3 Image Retrievers if our data connection is good.

After the download, an Image Retriever is responsible for two more tasks.
First, it has to call the Image Store component
(\labelindexref{Section}{sub-sec:im-store}) and send the image to it.
Secondly, it has to produce a message containing the image ID (image URL) and
write it to the Kafka queue on which the Detectors await.

\subsection{Image Store}
\label{sub-sec:im-store}

The Image Store component is basically an interface that is an abstraction of
the storage engine used underneath. It represents a one common entity that
knows where images are stored and can do two types of operations:
\texttt{putImage(imageID, imageBytes)} and \texttt{getImage(imageID)}.

As \labelindexref{Figure}{img:im-store} shows, in the current version of the
implementation, the Image Store component uses the disk, because most of the
images we deal with
need to be written and read only once in 99\% of the cases. In the future, if we
decide to move the whole system in the Amazon Cloud, the underneath storage
can be Amazon S3\footnote{\url{https://aws.amazon.com/s3/}}. For now, it is
sufficient to use the disk in order to store images and more important to
have the storage hidden behind a good abstraction layer such that other
components do not know who they are talking to, besides the Image Store
component.

\fig[scale=0.5]{src/img/im-store.pdf}{img:im-store}{Image Store Component}
\subsection{Detector Module}
\label{sub-sec:detector}

This module is the most computationally intensive one and requires more than
90\% of the whole processing power available. In our logo recognition
situation, these processes are the bottleneck for the entire system, so if we
were to look for performance optimizations this should have been the first place to
check. However, we designed the system such that it is entirely algorithmic
agnostic, so in a situation where a different, less intensive, algorithm is
run inside those Detectors, the situation may change, at least with regard to
the bottlenecking problem. Truth be told, this would be very unlikely, because
image processing tasks are computationally intensive by their nature.

A Detector process pulls its data from the Kafka queue where Image Retrievers
write imageIDs, in our case image URLs. Once a Detector has the image URL, it
requests the actual image from the Image Store
(\labelindexref{Section}{sub-sec:im-store}) and starts processing it. The
processing consists in calling an abstract interface that represents an
algorithm. One such interface takes the image as parameter and outputs a list
of found features of the image, in our case logos. This list of found features
is then compacted in a message and quickly wrote to the next Kafka queue on
which the Annotated Data Storage Module
(\labelindexref{Section}{sub-sec:ads-module}) awaits. This workflow is
better described in \labelindexref{Figure}{img:im-detector} below. As the figure
shows, the detectors do not rely on a specific algorithm and can
run any type of processing, providing they are fed up with the implementation.
We tried to illustrate this idea in the two lightly colored boxes.

In our type of implementation, with the Detectors being actually logo
Detectors, there is also a need to store somewhere the known logos. Our logo
detection algorithm (see \labelindexref{Section}{sec:logo-alg}) does not
recognize logos out of nowhere, it rather compares them to a list of known
logos. The design decision we took in this case, at least for now, was to
bundle the list of images with the logo detection algorithm such that the
process will have them in the \texttt{resources/} folder. The downside of this
approach is, of course, the fact that every time a logo needs to be updated we
have to rebuild the Detector and re-run it. The good part is that this process
is very easy to do, we just have to add the new set of images to a folder
and recompile the program. Moreover, we only need to do this operations only
on one subset of detectors, namely 3 of them. We chose 3 as the magic number
here in order to have triple replication. For more details about the queueing
system and how the Detectors are organized, please follow
\labelindexref{Section}{sub-sec:q-system}.

\fig[scale=0.5]{src/img/im-detector.pdf}{img:im-detector}{Detector Workflow}

\subsection{Annotated Data Storage Module}
\label{sub-sec:ads-module}

At this moment we find ourselves beyond the Detector wall and the volume of
incoming traffic has diminished considerably, because at this stage we only
have \textit{interesting} images from the logical, human point of view. These
images are about 1\% of the total images analyzed, so, if we assume that the
Detector cohorts can keep up with the 30 images pushed into the pipeline by
the Fetcher, the storage module has to face 1 image every 3--4 seconds on
average. This calculation proves that we only need one process of this kind.
In case of an unlikely event of a hardware of software failure, we only have
to restart the component. The good part is that we are again backed up by the
messaging system that caches the messages until the process goes back up, so
we do not lose precious analyzed and found positive data.

Mostly because our messages are basically JSON messages, but also because of
reliability, we chose MongoDB\footnote{\url{https://www.mongodb.org}} as the
database for our system. The storage module is also agnostic about the type of
computation we did in the previous layers, so the data that gets stored is
nothing more than the image URL and the list of features found in the image.
We call this features \textit{annotations}, as the
\labelindexref{Listing}{lst:mongo-json} shows.

\lstset{caption=MongoDB Document,label=lst:mongo-json}
\begin{lstlisting}
  {
     "imageURL":    "http://example.com",
     "annotations": [ "Honda",
                      "Ford",
                      "Mazda",
                    ]
  }
\end{lstlisting}

The database is also polled once every 10 seconds by the simple web dashboard
in order to check for new logo occurrences and compute new statistics. Because
of the rare frequency of the event when we identify a known logo, there is very
little load on the database, so we only need one small server to keep it
running. In terms of replicas, we rely on the internal MongoDB replications
and failure recovery mechanisms.

\subsection{Messaging and Queing System}
\label{sub-sec:q-system}

As stated before, we rely on the Apache Kafka when it comes to passing
messages around and exchange data between different components. Each component
is connected to the next one by a Kafka channel (topic) through which only the
same type of messages should be sent. Kafka does not have a mechanism of data
introspection, such that we can discard potentially malformed messages came from
defective processes, thus we are obliged to do sanity checks every time we
read a message.

The logo Detectors in our implementation are very slow,
mainly because they have to check an image against a list of logos, not just
one single known logo. So, if we were to keep adding logos to the known
database, we would have ended up in the situation where a single logo Detector
needs minutes or, even worse, hours, to check only one image. Imagine that a
check against a single logo needs about
1-2 seconds. If we have 1000 known logos, the Detector needs 1000-2000
seconds,  which roughly means half an hour. By any definitions of
benchmarking, evaluations or even the common sense, that is a lot of
time.

Apart from the main abstraction, the topic, Kafka provides another abstraction, the
\textit{partition} (see \labelindexref{Section}{sub-sec:kafka}). Because Kafka
provides these two powerful abstractions, we make use of them especially in
the case of Detectors. To avoid the above described behaviour, we split all
available Detectors in detector groups, like
\labelindexref{Figure}{img:detector-groups} shows. This idea allows two
important things, the first one being this problem with computation taking too
much for one photo. It is true that by this method we do not diminish the
overall needed computational time, i.e. in our case we still have 9 logos to
test against, but we diminish the time needed per process, i.e. one Detector
has to test against 3 instead of 9 logos. Besides the advantage of having a
fair work load distribution, there is also another perk in having this
Detector organization, namely when adding a new known logo: we only have to do
it for one group of Detectors (see \labelindexref{Section}{sub-sec:detector}).

One problem that is avoided by Kafka's design is the race between Detectors in
the same Detector group. If Kafka did not offer a guarantee that messages
wrote in the same partition are ordered and read by exactly one consumer, we
would have needed to solve the race condition by manually implementing a locking
mechanism between Detectors from the same group which would have complicated
the whole system. So, once again, Kafka proved itself a suitable choice in
this situation.

Messages that we send through Kafka topics from one component to another are
serialized as JSON documents. Until now, mainly because of the incipient stage
of the project, the messages are very small and have 2-3 keys, so we did not
feel the need of using a compression library, like
zLib\footnote{\url{http://www.zlib.net}}, therefore we send messages in clear
text. In terms of numbers, we are currently using 3 Kafka topics and 3 types
of messages. The 3 topics are: \texttt{toDownload}, \texttt{toDetect} and
\texttt{toStoreDetected} and the 3 types of messages are:
\texttt{DownloadMessage}, \texttt{DetectMessage} and
\texttt{StoreDetectedMessage}. The first two types of messages consist only in
the imageID (image URL in our case), whereas the last one has an additional
list of annotated data (found logos) which is, in most cases, empty.

\fig[scale=0.5]{src/img/detector-groups.pdf}{img:detector-groups}{Detector
Groups}

\section{Logo Recognition Algorithm}
\label{sec:logo-alg}

As stated before, the general idea of this project was to create a processing
pipeline that allows any type of image computations done on images fetched
from social networks streaming endpoints. In order to have a fully functional
complete implementation, we also had to choose one algorithm and apply it on
the input data. Therefore, we chose a Logo Recognition algorithm that attempts
to find logos in a random image based on a database of known logos. Because
the main requirement was not the algorithm, but because we needed a fairly
fast and simple one, we chose the Template Matching Approach (see
\labelindexref{Section}{sub-sec:template-matching}).

The main idea of this approach is to obtain a template from the known logos
and try to match it in every possible position of the query image and it works
very well for images that have multiple colors and rich textures or details.
Unfortunately, logos are most of the time monocolor and, to worsen things up,
without any distinguishable details, not even gradients, i.e. the Twitter
logo (see \labelindexref{Figure}{img:8-logos}). The Twitter logo is basically
a normal, weird shaped light blue spot. With this in mind, if we try to match
it against a completely blue image (same blue color) it would perfectly match
in every location. A na{\"i}ve algorithm would even find thousands of matches
in the given image. To solve this problem we had to come up with an original
approach and this is: split the image into 4 equally sized squares, match
each one of the squares independently and, finally, compare the \textit{best
matched} positions and see if they are in the same order as they were in the
template. \labelindexref{Figure}{img:4-squares} attempts to explain it better.
We have the Twitter logo template, one random picture containing the logo,
the 4-way split template and the matched template over the query image.

As the figure shows, the 4 matched squares are approximately in the same
relative position as in the original template, thus, according to this
algorithm standards, is a strong belief that the logo have been identified in
that position. One thing to notice, though, is that the matches are not in
\fig[scale=0.5]{src/img/4-squares.pdf}{img:4-squares}{4-way matching
algorithm}
exactly the same relative position, and this is mainly caused by the matching
function used (see \labelindexref{Section}{sub-sec:template-matching}) which
does an approximation. Throughout our implementation we chose the
\texttt{CV_TM_CCORR} metric, because it proved itself to be the most accurate
in the experiments we have run. Another important thing, that we have also
determined by running multiple experiments and doing the average, was setting
the maximum threshold distance allowed between two squares, both horizontally
and vertically. To be more precise, there is a 20\% (of the square size)
allowance between the upper row squares and lower row squares and 31\% between
right and left. Any match that \textit{looks like} the template, but has
larger distances between its squares, is considered a false match. However, we
have also determined experimentally that, in some cases, 3 of the squares
perfectly match themselves onto the query image (less than 5\% gap between
them), but one of them which matches in a totally different place. In such
cases, we consider the situation a bad luck event and still return a correct
match with the condition that the 3 squares that match in the same place have
to comply with the 20\% and 31\% boundaries.
\labelindexref{Listing}{lst:4-way-alg} is the pseudocode of
the 4-way split algorithm that runs inside a Detector process.

\lstset{language=Python,caption=4 way splitting template matching algorithm,label=lst:4-way-alg}
\begin{lstlisting}
def match(template_image, query_image):
  squares = ['red', 'yellow', 'blue', 'green']
  for square in squares:
    template_pos[square] = 4way_split(square, template_image)

  for square in squares:
    best_match[square] = find_best_match(template_pos[square], template, query_image)

  sq_size_x = template_image.x / 2
  sq_size_y = template_image.y / 2
  matches_count = 0

  if x_difference(best_match['red'], best_match['yellow']) > 0.31 * sq_size_x:
    matches_count += 1

  if x_difference(best_match['blue'], best_match['green']) > 0.31 * sq_size_x:
    matches_count += 1

  if y_difference(best_match['red'], best_match['blue']) > 0.20 * sq_size_y:
    matches_count += 1

  if y_difference(best_match['yellow'], best_match['green']) > 0.20 * sq_size_y:
    matches_count += 1

  if matches_count >= 3:
    return true
  return false
\end{lstlisting}

\labelindexref{Listing}{lst:4-way-alg} presents only the general idea of the
algorithm. In reality, we have to do all sorts of other checks, like, for
example, check if the squares respect the distance threshold with their
neighbours lest they are actually mirrored. For example, the checks in
\labelindexref{Listing}{lst:4-way-alg} do pass in the event when the red and
yellow squares are on the lower row instead of the upper.

A problem that is not very hard to see with this situation is what happens if
the template has a difference size compared to the logo in the query image,
say 10 times bigger. In this case, it is obvious that the result will be
most likely false because even one square would still be 5 times larger that
the whole logo. To solve this problem we do a multiple scale scan, with the
template size decreased in exponential steps until we find the correct match
or until the template becomes to small. To be more precise, we start with the
template size approximately equal with the query image and do a full round
trip (split the template, match each square separately etc). After that, we
decrease the size of the template by 1.15 (determined experimentally) and redo
the process. In practice, it turned out that we need between 12--17 round trips
to do a complete check. This process is represented in
\labelindexref{Listing}{lst:multiple-scale}.

\lstset{language=Python,caption=Multiple scale template
matching,label=lst:multiple-scale}
\begin{lstlisting}
def match_one(template, query):
  if template.x + template.y < 50px:
    return false

  if template.x > query.x or template.y > query.y:
    return false

  if match(template, query):
    return true
  else:
    return match_one(shrink(template, 15%), query)
\end{lstlisting}

One last shortcoming of this implementation is that it is very likely to
produce wrong results in the event when the same logo can be found more than
once inside the query image. The reasons why this is happening are pretty
obvious: two squares can match on one logo and the other two squares on
another one, the final result being false. So, not only we miss one occurrence
of the logo, but we miss all of them. Fortunately for us, the experiments
proved that this situation happens so rarely in practice that is far from
being a concern right now.
