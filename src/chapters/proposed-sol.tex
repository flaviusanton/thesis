\chapter{Proposed Solution}
\label{chapter:proposed-sol}

In this chapter we will see a detailed description of the system we have built
in the scope of this thesis. In \labelindexref{Section}{sec:reqs-tech} we
present what were the requirements and constraints of this project together
with what technologies we used and why we chose them. In the next part of this
chapter, \labelindexref{Section}{sec:sys-arch}, we take each component in the
pipeline and present how it works. Finally, the third part,
\labelindexref{Section}{sec:logo-alg}, consists in the detailed description of
the template matching algorithm variation that we used for the logo
recognition component.

We will shortly see in the next section that one of the biggest requirements
(and achievements for that matter) of this thesis was to build a system that
is algorithm agnostic and can do any type of computation on images fetched
from social media streams. In order for this to happen, it was necessary to
divide the work into multiple separate components each one being responsible
for a very specific task, i.e. fetching images, storing images, recognizing
logos etc. But before diving into each component, let us see what were the
requirements and technologies used in building this system.

\section{Requirements and Technologies Used}
\label{sec:reqs-tech}

Even though the project is open source and we had almost full freedom in
designing the system and choosing the technologies, we still had to conform
with some requirements came from the proposing company, Hootsuite. One of the
rigid requirements was that the project has to be written in Scala, not
necessarily because they considered Scala is the best choice here, but because
most, if not all, of their platform is built upon Scala. More details about
Scala, what are its advantages, disadvantages or why it is fun to use, can be
found in \labelindexref{Section}{sub-sec:scala}.

Another requirement that did not come from Hootsuite, but rather from the
common sense is that the system has to scale very easily. Like we saw in
\labelindexref{Section}{sec:proj-motivation}, the number of images that this
system has to face is quite big, even with the computation power we have
available nowadays. The most intuitive way to imagine a scalable system is if
we add an arbitrary number of machines, let's say \(M_{new}\), than the task has to
completed \(1 + \frac{M_{new}}{M_{old}}\) times faster, where \(M_{old}\) is the old number
of machines available. In practice, mostly due to Amdahl's
Law\footnote{\url{http://home.wlu.edu/~whaleyt/classes/parallel/topics/amdahl.html}},
but also because of other factors, like network latency, hardware or software
fails etc, the system will never reach that idealized speedup, but if the
speed gain is linear with \(1 + \frac{M_{new}}{M_{old}}\), we can declare that the
system is scalable. Fortunately for us, the task we have to perform is very
easy to distribute across multiple machines. Suppose that we have \(M\)
machines and each of them is running an instance of the logo recognition
algorithm, then if we have \(2 \times M\) images to process we can easily
distribute \(2\) images per machine and then collect the results in the end.
Because images that come from the social media streaming endpoints are rather
independent and processing them represent perfectly independent tasks, we do
not even need to collect the results, but this is a discussion we will have in
\labelindexref{Section}{sec:sys-arch}.

One of the main characteristics of the system is that it has to allow
introducing new \textit{known} logos or removing old ones in an easy manner.
It is allowed to have a small delay of a few minutes before the system can
recognize the new logo when we want to insert it
int the \textit{known logos database}, but not hours or, even worse,
days. By easy we mean here fast and reliable rather than easy from the end user point of
view. The actual process is fine to involve digging into the filesystem,
updating some directory, restarting some components, but this process has to
be seamless from the whole system perspective, we cannot afford shutting
everything down and restarting just because we want to insert a new logo
because the service has to run continuously.

The process of moving an image through the pipeline from one end to the other
should finish either with a positive result (i.e. we found this list of logos
in the image) or with a negative one. In the former case, one of the
components has to store the image (or the image link) in a database together
with some data to describe the result, for example the logo list. This type of
metadata is often called \textit{annotated data} along this thesis and we may
also refer to the pair \((image, logo list)\) as the \textit{annotated
image}.

Despite being designed as mostly a background job that sometimes stores
results in a database, our system have to have at least a minimum interaction
with a human and we decided that interaction to be in the form of a minimal
web interface where one can basically see the pairs \((image, logo list)\)
from the database, but in a nicer form. Also, the dashboard has to offer at
least a table or chart with statistics regarding recognized logos. To increase
the usefulness of the dashboard it should be real time and compute the
statistics as long as new logos are discovered. It is fine to have a slight
delay in updating the dashboard, but the web application should poll the
database at least once a minute.

Because our focus is rather on the infrastructure and reliability rather than
the algorithm approach itself which can be changed at any future time, but
also because we cannot miss important data and have false statistics, the system
is not allowed to lose messages. In the scope of this thesis, by
\textit{message} we understand the output of one component and the input of
the next component in the pipeline. Of course, the first component, Fetch
Module (see \labelindexref{Section}{sub-sec:fetch-module}) and the last
component, Annotated Data Storage Module (see
\labelindexref{Section}{sub-sec:ads-module}), make an exception from this rule
as they only output messages, in the case of the former, or consume messages, if we speak
about the latter. That does not mean that the first component has no input, it
means that its input comes directly from the outside world and represent
unstructured data, i.e. tweets or posts, streamed by the Social Networks.
Because the volume of steamed data can become very large, we have to make sure
that all this data gets processed on time by our system, so in order for this
to happen, the first component has to quickly get rid of the posts and pass
the work to the next layers.

When it comes to passing messages around, we needed a good infrastructure that
supports not necessarily large amounts of data, but a tool that allows multiple producers,
multiple consumers and does not offer the same message twice for processing, because we do not
want the same image to be analyzed multiple times. We say that we
do not need large amounts of data, because we do not have to send entire
images between components. We describe in
\labelindexref{Section}{sub-sec:im-store} the way we store images
so that each component can fetch them directly from the centralized storage, thus avoiding
passing them around each time we send a message. Therefore, our messaging
system has to be reliable, seamlessly scalable and have a fast way of
partitioning the data channels, because we do not want to mix messages around,
we only need one channel between two types of components without allowing
other processes to have access to that data. One of the mature, distributed
open source, messaging systems available is Apache
Kafka\footnote{\url{http://kafka.apache.org}}.

\subsection{Kafka -- A Messaging System}
\label{sub-sec:kafka}

\todo{Talk about general Kafka architecture, why is it good in this situation}

\subsection{Scala}
\label{sub-sec:scala}

\todo{General stuff about Scala. Advantages/disadvantages}

\section{System Architecture}
\label{sec:sys-arch}

In this section we explore the system architecture as a whole and then we go
one level up and explore each separate component.
\labelindexref{Figure}{img:hl-arch} shows a general overview of the whole
pipeline. As shown, data comes from the social networks and enters the
pipeline through the Fetcher Module which filters every post or tweet and
writes the output to a Kafka topic (see
\labelindexref{Section}{sub-sec:kafka}) from which several processes read and
download the images. The downloaded data then gets sent by the Retrievers to
the centralized Image Store from which the Detectors pull images for
processing purposes. The Detectors are also connected with the Retrievers by a
Kafka topic. After the detection process, the life of an image can stop, in
case nothing was detected, or can continue to the Storage Module which talks
to a MongoDB\footnote{\url{https://www.mongodb.org}} database. The database
gets polled periodically by a web application which can be accessed directly by a human
user.

\fig[scale=0.35]{src/img/hl-arch.pdf}{img:hl-arch}{High Level System
Architecture}

Even though some of the components may seem that could have been very well
integrated into other ones (for example Image Retriever into Fetcher), we will
shortly see that if we do some computations there is impossible to use only
one machine for some of them, in our case for the Image Retriever component.
Therefore, the reason why some of the modules are drawn as duplicated
rectangles is to emphasize that they are replicated processes, so not
necessarily need to be run on different machines, but there is a need for more
instances of them. The replication is not needed, in this case, for
reliability and fault tolerance purposes, but in order to increase the power
of consuming more incoming data.

Apart from the replicated components, \labelindexref{Figure}{img:hl-arch}
shows another strange characteristic: there are two piles of Logo Detectors.
By this, we only try to show that some detectors are specialized in recognizing
some logos, while other pile of detectors are specialized in another set of
logos. We took this decision because the process of testing an image against
each and every possible logo in the database would take too long on a single
detector, so it is better to have an even work distribution. Moreover, the most
important reason we chose to do this is that, if we want to add a new logo in
the equation, we only have to restart one pile of detectors, which may be
considerably faster than restarting all detectors.

As we have seen in \labelindexref{Section}{sub-sec:kafka}, the messaging
system we chose for our implementation can keep messages for as long as they
are needed and producers or consumers can come and go at any given time
without impacting the stored data. To use this to our advantage, each
component basically runs an infinite loop where it waits for input, do some
computation and writes to the corresponding queue towards the next component
in the chain, as the code in \labelindexref{Listing}{lst:comp-workflow} shows.

\lstset{caption=General Component Workflow,label=lst:comp-workflow}
\begin{lstlisting}
initializeComponent();
consumer = new KafkaConsumer(INPUT_TOPIC);
producer = new KafkaProducer(OUTPUT_TOPIC);

while (true)
  message = consumer.consumeOne();
  result = process(message);
  producer.produce(result);
\end{lstlisting}

Having this type of workflow for each component is also very convenient from
the update or upgrade perspective. Basically, each component is a
stateless process that takes some input, applies a function that does
not need any state and then returns an output. The only entity responsible for
keeping the state is Kafka, our distributed messaging system, so fortunately
for us, we can turn down and up any component at any given time. In a
hypothetic situation where all Detectors have failed and went down, the system
is well able to recover if the Detectors restart, providing there is enough disk space
to cache all those incoming messages and images. However, disk have become so
cheap nowadays that we can afford having a lot of it, such that the probability of
complete system fail (data loss) is very low.

As far as component restarts concern, they are also needed in case of a new
logo being added to the known logo database or in case of a code base update.
It is true that in the latter situation, we also have to make sure the changes
are compatible, but the biggest concern should be how do we keep the whole
system running, in spite of updating some of the components. This problem is
also automatically solved by the fact that our clever messaging system is
caching for as long as we need it to. If a consumer simply goes down for
whatever reason, Kafka will notice that, but there is no action required as
the remaining consumers will simply take that workload. If there is no
consumer left or too few of them, Kafka will cache the messages until there
is again a sufficient number of consumers to continue working.

That being said, in the following sections, we present each separate component that takes part
into the processing pipeline.

\subsection{Fetch Module}
\label{sub-sec:fetch-module}

The Fetch Module is the frontend of our system as it is the only component
that connects to the outside world. It is responsible for fetching streams of
unstructured data, i.e. posts, tweets etc, filtering this data in order to
keep only what is of interest to us, namely images and downloading these
images. After that, the downloaded image gets stored by the Image Store module
and the Detectors are notified by the incoming message in the corresponding
Kafka queue. This is what happens in a few words, but let us explore some
of the subtleties of this module.

First of all, there is no need to have multiple machines, nor processes that
are responsible for fetching posts or tweets from the streaming endpoints. To
back this up, we make a reference to
\labelindexref{Section}{sec:proj-objectives} and the calculations we did
there. If we sum all the incoming tweets and posts we barely reach around 250
messages per second. Given the fact that images are not included as data
inside the tweets, but as links and we know that a tweet has at most 140
characters (let us assume that all the data comes from Twitter and a link has
at most 100 bytes for simplicity) simple math yields that this component has
to face 60KB per second (\((140B + 100B) \times 250 = 60KB\)), a value that
was modest even 10 years ago.

One may simply ask what happens with this component in case of hardware or
software failures and it is a valid question. Well, the answer is so simple
that it may be shocking at the first glance: nothing happens immediately. By
this, we do not mean that the system stops working entirely, but for a period
of time, the fetch component will simply be down and there will be no new
tweets fetched. We chose to do this mainly for two reasons. First, because you
cannot have multiple connections to the same social network endpoint. The
Twitter APIs, for example, limit the number of connections at 1 per
application, so there is no point in having replicas for this component.
Second, because even if we would have additional replicas being ready to jump in when the
main replica fails, there is still a number of streamed tweets that will be
lost forever. With this in mind and the fact that Twitter sends us only 1\%
of their data, there is nothing to worry about if we lose a number of streamed
tweets, because we do not have control of which tweets they stream. Those
\(N\) tweets that we presumably cannot afford to lose could have been in the
other 99\% from the beginning. The most important thing about our system is to
keep the Detectors busy all the time, because they are the real bottleneck of
the whole pipeline, even if the Detectors have the biggest number of machines
assigned. Due to cost limitations, (see
\labelindexref{Section}{sec:cost-and-resource}) we periodically need to shut
down the Fetch Module, to allow the Detectors to keep up with all the incoming
data.

In the future, if the hardware power is increased considerably, there is also
the possibility of adding dedicated Fetcher replicas that are ready to jump in
the game in case of a failure, but for now this is not a big concern.
Moreover, if we add more social networks to the system such that one single
Fetcher has to download from too many endpoints, we might also change the
design and split the work between multiple Fetchers.

As we have stated before, the Fetcher is only responsible for getting the
stream of data and filter the images and by filtering means discarding the
whole message content but the image URL. The image URL is the only thing that
gets past the Fetcher and gets written in a corresponding Kafka queue on which
more Image Retriever processes await.

It should be clear to the reader, by now, that the Fetcher and Image Retriever
(see \labelindexref{Figure}{fig:hl-arch}) are two separate processes, but they
do not need to run on different machines. Because the processing power needed here is
very low, but also because these two components are very bound to each other
in terms of logic, we are considering these two types of processes as
being one single component. Even though there is a Kafka queue between them,
this is used more as a caching level rather than as a logical boundary.

The Image Retriever processes are, therefore, replicated mostly regarding
failure resistance, but also because of bandwidth. If we know that an image is
approximately 50--200KB and we have about 30 images per second to process that
gives us \(30 \times 200KB = 6MB\) of data to download per second in the worst
case. If we also take peaks due to busy hours into consideration, the amount
of data to download in one second can exceed 10MB so we do not want all this
incoming traffic on one single machine. This is the main reason we should at
least 2 or 3 Image Retrievers if our data connection is good.

After the download, an Image Retriever is responsible for two more tasks.
First, it has to call the Image Store component
(\labelindexref{Section}{sub-sec:im-store}) and send the image to it.
Secondly it has to produce a message containing the image ID (image URL) and
write it to the Kafka queue on which the Detectors await.

\subsection{Image Store}
\label{sub-sec:im-store}

The Image Store component is basically an interface that is an abstraction of
the storage engine used underneath. It represents a one common entity that
knows where images are stored and can do two types of operations:
\texttt{putImage(imageID, imageBytes)} and \texttt{getImage(imageID)}.

As \labelindexref{Figure}{img:im-store} shows, in the present future we simply
need to be wrote and read only once in 99\% of the cases. In the future, if we
decide to move the whole system in the Amazon Cloud, the underneath storage
can be Amazon S3\footnote{\url{https://aws.amazon.com/s3/}}. For now, it is
use the disk in order to store images. This is mainly because they currently
good to have the storage hid behind a good abstraction layer such that other
\fig[scale=0.5]{src/img/im-store.pdf}{img:im-store}{Image Store Component}
components do not know who they are talking to.

\subsection{Detector Module}

This module is the most computationally intensive one and requires more than
90\% of the whole processing power available. In our logo recognition
situation, these processes are the bottleneck for the entire system so if we
were to look for performance optimizations this should be the first place to
check. However, we designed the system such that it is entirely algorithmic
agnostic, so in a situation where a different, less intensive, algorithm is
run inside those Detectors, the situation may change, at least with regard to
the bottlenecking problem. Truth be told, this would be very unlikely, because
image processing tasks are computationally intensive by their nature.

A Detector process pulls its data from the Kafka queue where Image Retrievers
write imageIDs, in our case image URLs. Once a Detector has the image URL, it
requests the actual image from the Image Store
(\labelindexref{Section}{sub-sec:im-store}) and starts processing it. The
processing consists in calling an abstract interface that represents an
algorithm. One such interface takes the image as parameter and outputs a list
of found features of the image, in our case logos. This list of found features
is then compacted in a message and quickly wrote to the next Kafka queue on
which the Annotated Data Storage Module
(\labelindexref{Section}{sub-sec:ads-module} awaits. This is workflow is
better described in \labelindexref{Figure}{img:im-detector} below. As the figure
shows, the detectors do not rely on a specific algorithm and can
run any type of processing, providing they are fed up with the implementation.
We tried to illustrate this idea in the two lightly colored boxes.

In our type of implementation, with the Detectors being actually logo
Detectors, there is also a need to store somewhere the known logos. Our logo
detection algorithm (see \labelindexref{Section}{sec:logo-alg}) does not
recognize logos out of nowhere, it rather compares them to a list of known
logos. The design decision we took in this case, at least for now, was to
bundle the list of images with the logo detection algorithm such that the
process will have them in the \texttt{resources/} folder. The downside of this
\fig[scale=0.5]{src/img/im-detector.pdf}{img:im-detector}{Detector Workflow}
approach is, of course, the fact that every time a logo needs to be updated we
have to rebuild the Detector and re-run it. The good part is that this process
is very easy to do, we just have to add the new set of images to a folder
and recompile the program. Moreover, we only need to do this operations only
on one subset of detectors, namely 3 of them. We chose 3 as the magic number
here in order to have triple replication. For more details about the queueing
system and how the Detectors are organized, please follow
\labelindexref{Section}{sub-sec:q-system}.

\subsection{Annotated Data Storage Module}
\label{sub-sec:ads-module}

At this moment we find ourselves beyond the Detector wall and the volume of
incoming traffic has diminished considerably, because at this stage we only
have \textit{interesting} images from the logical, human point of view. These
images are about 1\% of the total images analyzed, so, if we assume that the
Detector cohorts can keep up with the 30 images pushed into the pipeline by
the Fetcher, the storage module has to face 1 image every 3--4 seconds on
average. This calculation proves that we only need one process of this kind.
In case of an unlikely event of a hardware of software failure, we only have
to restart the component. The good part is that we are again backed up by the
messaging system that caches the messages until the process goes back up, so
we do not lose precious analyzed and found positive data.

Mostly because our messages are basically JSON messages but also because of
reliability we chose MongoDB\footnote{\url{https://www.mongodb.org}} as the
database for our system. The storage module is also agnostic about the type of
computation we did in the previous layers, so the data that gets stored is
nothing more than the image URL and the list of features found in the image.
We call this features \textit{annotations}, as the
\labelindexref{Listing}{lst:mongo-json} shows.

\lstset{caption=MongoDB Document,label=lst:mongo-json}
\begin{lstlisting}
  {
     "imageURL":    "http://example.com",
     "annotations": [ "Honda",
                      "Ford",
                      "Mazda",
                    ]
  }
\end{lstlisting}

The database is also polled once every 10 seconds by the simple web dashboard
in order to check for new logo occurrences.

\subsection{Messaging and Queing System}
\label{sub-sec:q-system}

\todo{Explain how messages get passed around. Talk about message types. What
data do we send. How do components fetch images without sending them around.}

\section{Logo Recognition Algorithm}
\label{sec:logo-alg}
\todo{Explain the algorithm. Talk about the general idea. Why is it good in
our situation.}
